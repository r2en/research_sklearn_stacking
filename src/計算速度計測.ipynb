{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', 1000)\n",
    "pandas.set_option('display.max_columns', 1000)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "import pandas\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dataset():\n",
    "    \n",
    "    train = pandas.read_csv('train.csv')\n",
    "    test = pandas.read_csv('test.csv')\n",
    "    \n",
    "    datasets = [train, test]\n",
    "    \n",
    "    def get_title(name):\n",
    "        if re.search(' ([A-Za-z]+)\\.', name):\n",
    "            return re.search(' ([A-Za-z]+)\\.', name).group(1)\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "    for dataset in datasets:\n",
    "\n",
    "        dataset['Cabin'] = dataset['Cabin'].apply(lambda x: 1 if type(x) == str else 0)\n",
    "        \n",
    "        dataset['Age'] = dataset['Age'].fillna(-1).astype(int)\n",
    "        \n",
    "        dataset['Fare'] = dataset['Fare'].fillna(-1).astype(int)\n",
    "\n",
    "        dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "        \n",
    "        dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "        dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "        title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "        dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "        dataset['Title'] = dataset['Title'].fillna(-1)\n",
    "\n",
    "        dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "        dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "        dataset.drop(['PassengerId', 'Ticket', 'Name'], axis=1, inplace=True)\n",
    "        \n",
    "    X_train = train.drop(['Survived'], axis=1)\n",
    "    y_train = train['Survived']\n",
    "    X_test = test\n",
    "    \n",
    "    std = StandardScaler()\n",
    "    std.fit(X_train)\n",
    "    X_train = std.transform(X_train).astype(numpy.float32)\n",
    "    X_test = std.transform(X_test).astype(numpy.float32)\n",
    "\n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train}\n",
    "    \n",
    "df = first_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tTrain's binary_logloss: 0.464484\tTest's binary_logloss: 0.487243\n",
      "[40]\tTrain's binary_logloss: 0.408891\tTest's binary_logloss: 0.446751\n",
      "[60]\tTrain's binary_logloss: 0.393729\tTest's binary_logloss: 0.442872\n",
      "[80]\tTrain's binary_logloss: 0.378047\tTest's binary_logloss: 0.436778\n",
      "[100]\tTrain's binary_logloss: 0.37472\tTest's binary_logloss: 0.436104\n",
      "[120]\tTrain's binary_logloss: 0.365463\tTest's binary_logloss: 0.432786\n",
      "[140]\tTrain's binary_logloss: 0.35433\tTest's binary_logloss: 0.424344\n",
      "[160]\tTrain's binary_logloss: 0.35888\tTest's binary_logloss: 0.426459\n",
      "[180]\tTrain's binary_logloss: 0.337339\tTest's binary_logloss: 0.416787\n",
      "[200]\tTrain's binary_logloss: 0.324049\tTest's binary_logloss: 0.413225\n",
      "[220]\tTrain's binary_logloss: 0.317539\tTest's binary_logloss: 0.415004\n",
      "[240]\tTrain's binary_logloss: 0.308851\tTest's binary_logloss: 0.415523\n",
      "[260]\tTrain's binary_logloss: 0.302299\tTest's binary_logloss: 0.416495\n",
      "[280]\tTrain's binary_logloss: 0.2962\tTest's binary_logloss: 0.41336\n",
      "[300]\tTrain's binary_logloss: 0.29189\tTest's binary_logloss: 0.411006\n",
      "[320]\tTrain's binary_logloss: 0.287122\tTest's binary_logloss: 0.409593\n",
      "[340]\tTrain's binary_logloss: 0.285989\tTest's binary_logloss: 0.409112\n",
      "[360]\tTrain's binary_logloss: 0.289834\tTest's binary_logloss: 0.408724\n",
      "[380]\tTrain's binary_logloss: 0.285252\tTest's binary_logloss: 0.410179\n",
      "[400]\tTrain's binary_logloss: 0.280402\tTest's binary_logloss: 0.412111\n",
      "[420]\tTrain's binary_logloss: 0.277603\tTest's binary_logloss: 0.412522\n",
      "[440]\tTrain's binary_logloss: 0.271024\tTest's binary_logloss: 0.413539\n",
      "[460]\tTrain's binary_logloss: 0.267041\tTest's binary_logloss: 0.412336\n",
      "[480]\tTrain's binary_logloss: 0.256236\tTest's binary_logloss: 0.411442\n",
      "[500]\tTrain's binary_logloss: 0.256264\tTest's binary_logloss: 0.411502\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df['X_train'], df['y_train'], test_size=0.2, random_state=0)\n",
    "\n",
    "train_dataset = lgbm.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "test_dataset = lgbm.Dataset(data=X_valid, label=y_valid, free_raw_data=False)\n",
    "final_train_dataset = lgbm.Dataset(data=df['X_train'], label=df['y_train'], free_raw_data=False)\n",
    "\n",
    "lgbm_params = {\n",
    "    'boosting': 'dart', \n",
    "    'application': 'binary',\n",
    "    'learning_rate': 0.05,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'feature_fraction': 0.7,\n",
    "    'num_leaves': 41,\n",
    "    'metric': 'binary_logloss',\n",
    "    'drop_rate': 0.15\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "clf = lgbm.train(train_set=train_dataset,\n",
    "                 params=lgbm_params,\n",
    "                 valid_sets=[train_dataset, test_dataset], \n",
    "                 valid_names=['Train', 'Test'],\n",
    "                 evals_result=evaluation_results,\n",
    "                 num_boost_round=500,\n",
    "                 early_stopping_rounds=100,\n",
    "                 verbose_eval=20\n",
    "                )\n",
    "                \n",
    "clf_final = lgbm.train(train_set=final_train_dataset,\n",
    "                      params=lgbm_params,\n",
    "                      num_boost_round=500,\n",
    "                      verbose_eval=0\n",
    "                      )\n",
    "\n",
    "y_pred = numpy.round(clf_final.predict(df['X_test'])).astype(int)\n",
    "\n",
    "passengerId = pandas.read_csv('test.csv')['PassengerId']\n",
    "dataframe = pandas.DataFrame({'PassengerId': passengerId, 'Survived': y_pred})\n",
    "\n",
    "dataframe.to_csv('submission_single_lgbm_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Regressor, Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with timer('LightGBM'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(preprocessor=first_dataset, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {'n_estimators': 100, 'max_features': 0.5, 'max_depth': 18, 'min_samples_leaf': 4, 'n_jobs': -1}\n",
    "rf_params = {'n_estimators': 125, 'max_features': 0.2, 'max_depth': 25, 'min_samples_leaf': 4, 'n_jobs': -1}\n",
    "rgf_params = {'algorithm': 'RGF_Sib', 'loss': 'Log'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def NuralNetClassifier(X_train, y_train, X_test, y_test=None):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
    "    y_pred = numpy.ravel(model.predict(X_test))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LightGBMClassifier(X_train, y_train, X_test, y_test=None):\n",
    "    lgbm_params = {\n",
    "        'boosting': 'dart', \n",
    "        'application': 'binary',\n",
    "        'learning_rate': 0.05,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.7,\n",
    "        'num_leaves': 41,\n",
    "        'metric': 'binary_logloss',\n",
    "        'drop_rate': 0.15\n",
    "    }\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    train_dataset = lgbm.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "    test_dataset = lgbm.Dataset(data=X_valid, label=y_valid, free_raw_data=False)\n",
    "    \n",
    "    final_train_dataset = lgbm.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    clf = lgbm.train(train_set=train_dataset,\n",
    "                     params=lgbm_params,\n",
    "                     valid_sets=[train_dataset, test_dataset], \n",
    "                     valid_names=['Train', 'Test'],\n",
    "                     evals_result=evaluation_results,\n",
    "                     num_boost_round=500,\n",
    "                     early_stopping_rounds=100,\n",
    "                     verbose_eval=0\n",
    "                    )\n",
    "    \n",
    "    clf_final = lgbm.train(train_set=final_train_dataset,\n",
    "                          params=lgbm_params,\n",
    "                          num_boost_round=500,\n",
    "                          verbose_eval=0\n",
    "                          )\n",
    "\n",
    "    y_pred = clf_final.predict(X_test)\n",
    "\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelsPipeline(\n",
    "    Classifier(estimator=LightGBMClassifier, dataset=ds, use_cache=False),\n",
    "    Classifier(estimator=NuralNetClassifier, dataset=ds, use_cache=False),\n",
    "    Classifier(estimator=RGFClassifier, dataset=ds, use_cache=False, parameters=rgf_params),\n",
    "    Classifier(estimator=ExtraTreesClassifier, dataset=ds, use_cache=False, parameters=et_params),\n",
    "    Classifier(estimator=RandomForestClassifier, dataset=ds, use_cache=False, parameters=rf_params),\n",
    "    Classifier(estimator=LogisticRegression, dataset=ds, use_cache=False),\n",
    "    Classifier(estimator=KNeighborsClassifier, dataset=ds, use_cache=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heamy single stacking model] done in 119 s\n"
     ]
    }
   ],
   "source": [
    "with timer('heamy single stacking model'):\n",
    "    stack_ds = pipeline.stack(k=10, seed=0, add_diff=False, full_test=True)\n",
    "    stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, use_cache=False)\n",
    "    y_pred = stacker.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = ModelsPipeline(\n",
    "    Classifier(estimator=LightGBMClassifier, dataset=stack_ds, use_cache=False),\n",
    "    Classifier(estimator=NuralNetClassifier, dataset=stack_ds, use_cache=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (log_loss): 0.3998561112420261\n",
      "Best Weights: [0.50000622 0.49999378]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heamy multiple stacking model] done in 20 s\n"
     ]
    }
   ],
   "source": [
    "with timer('heamy multiple stacking model'):\n",
    "    weights = pipeline2.find_weights(log_loss)\n",
    "    predictions = pipeline2.weight(weights).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN抜き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelsPipeline(\n",
    "    Classifier(estimator=LightGBMClassifier, dataset=ds, use_cache=False),\n",
    "    Classifier(estimator=RGFClassifier, dataset=ds, use_cache=False, parameters=rgf_params),\n",
    "    Classifier(estimator=ExtraTreesClassifier, dataset=ds, use_cache=False, parameters=et_params),\n",
    "    Classifier(estimator=RandomForestClassifier, dataset=ds, use_cache=False, parameters=rf_params),\n",
    "    Classifier(estimator=LogisticRegression, dataset=ds, use_cache=False),\n",
    "    Classifier(estimator=KNeighborsClassifier, dataset=ds, use_cache=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = ModelsPipeline(\n",
    "    Classifier(estimator=LightGBMClassifier, dataset=stack_ds, use_cache=False),\n",
    "    Classifier(estimator=LogisticRegression, dataset=stack_ds, use_cache=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heamy single stacking model] done in 64 s\n"
     ]
    }
   ],
   "source": [
    "with timer('heamy single stacking model'):\n",
    "    stack_ds = pipeline.stack(k=10, seed=0, add_diff=False, full_test=True)\n",
    "    stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, use_cache=False)\n",
    "    y_pred = stacker.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (log_loss): 0.39725201890402023\n",
      "Best Weights: [0.30616325 0.69383675]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:188: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heamy multiple stacking model] done in 12 s\n"
     ]
    }
   ],
   "source": [
    "with timer('heamy multiple stacking model'):\n",
    "    weights = pipeline2.find_weights(log_loss)\n",
    "    predictions = pipeline2.weight(weights).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "        'boosting': 'dart', \n",
    "        'application': 'binary',\n",
    "        'learning_rate': 0.05,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.7,\n",
    "        'num_leaves': 41,\n",
    "        'metric': 'binary_logloss',\n",
    "        'drop_rate': 0.15\n",
    "}\n",
    "keras_params = {'epochs': 10, 'batch_size': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn():\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(12, input_dim=9, activation='relu'))\n",
    "    clf.add(Dense(6, activation='relu'))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('lgb', lgbm.LGBMClassifier(**lgbm_params)),\n",
    "    #('nn', KerasClassifier(build_fn=build_fn, **keras_params)),\n",
    "    ('rgf', RGFClassifier(**rgf_params)),\n",
    "    ('et', ExtraTreesClassifier(**et_params)),\n",
    "    ('rf', RandomForestClassifier(**rf_params)),\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('lgb',\n",
       "                                LGBMClassifier(application='binary',\n",
       "                                               boosting='dart',\n",
       "                                               boosting_type='gbdt',\n",
       "                                               class_weight=None,\n",
       "                                               colsample_bytree=1.0,\n",
       "                                               drop_rate=0.15,\n",
       "                                               feature_fraction=0.7,\n",
       "                                               importance_type='split',\n",
       "                                               learning_rate=0.05, max_depth=-1,\n",
       "                                               metric='binary_logloss',\n",
       "                                               min_child_samples=20,\n",
       "                                               min_child_weight=0.001,\n",
       "                                               min_data_in_leaf=20,\n",
       "                                               min_split_ga...\n",
       "                                                     n_jobs=None, n_neighbors=5,\n",
       "                                                     p=2, weights='uniform'))],\n",
       "                   final_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                      dual=False,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      intercept_scaling=1,\n",
       "                                                      l1_ratio=None,\n",
       "                                                      max_iter=100,\n",
       "                                                      multi_class='auto',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='lbfgs',\n",
       "                                                      tol=0.0001, verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sklean single stacking model] done in 30 s\n"
     ]
    }
   ],
   "source": [
    " with timer('sklean single stacking model'):\n",
    "    clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "    clf.fit(df['X_train'], df['y_train'])\n",
    "    predictions = clf.predict(df['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('lgb',\n",
       "                                LGBMClassifier(application='binary',\n",
       "                                               boosting='dart',\n",
       "                                               boosting_type='gbdt',\n",
       "                                               class_weight=None,\n",
       "                                               colsample_bytree=1.0,\n",
       "                                               drop_rate=0.15,\n",
       "                                               feature_fraction=0.7,\n",
       "                                               importance_type='split',\n",
       "                                               learning_rate=0.05, max_depth=-1,\n",
       "                                               metric='binary_logloss',\n",
       "                                               min_child_samples=20,\n",
       "                                               min_child_weight=0.001,\n",
       "                                               min_data_in_leaf=20,\n",
       "                                               min_split_ga...\n",
       "                                                      final_estimator=LogisticRegression(C=1.0,\n",
       "                                                                                         class_weight=None,\n",
       "                                                                                         dual=False,\n",
       "                                                                                         fit_intercept=True,\n",
       "                                                                                         intercept_scaling=1,\n",
       "                                                                                         l1_ratio=None,\n",
       "                                                                                         max_iter=100,\n",
       "                                                                                         multi_class='auto',\n",
       "                                                                                         n_jobs=None,\n",
       "                                                                                         penalty='l2',\n",
       "                                                                                         random_state=None,\n",
       "                                                                                         solver='lbfgs',\n",
       "                                                                                         tol=0.0001,\n",
       "                                                                                         verbose=0,\n",
       "                                                                                         warm_start=False),\n",
       "                                                      n_jobs=None,\n",
       "                                                      passthrough=False,\n",
       "                                                      stack_method='auto',\n",
       "                                                      verbose=0),\n",
       "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sklean multiple stacking model] done in 28 s\n"
     ]
    }
   ],
   "source": [
    " with timer('sklean multiple stacking model'):\n",
    "    final_estimator = StackingClassifier(\n",
    "        estimators= [\n",
    "            ('lgb', lgbm.LGBMClassifier(**lgbm_params)),\n",
    "            ('lr', LogisticRegression())\n",
    "        ],\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "\n",
    "    clf = StackingClassifier(\n",
    "        estimators= [\n",
    "            ('lgb', lgbm.LGBMClassifier(**lgbm_params)),\n",
    "            #('nn', KerasClassifier(build_fn=build_fn, **keras_params)),\n",
    "            ('rgf', RGFClassifier(**rgf_params)),\n",
    "            ('et', ExtraTreesClassifier(**et_params)),\n",
    "            ('rf', RandomForestClassifier(**rf_params)),\n",
    "            ('lr', LogisticRegression()),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ],\n",
    "        final_estimator=final_estimator\n",
    "    )\n",
    "\n",
    "    clf.fit(df['X_train'], df['y_train'])\n",
    "    predictions = clf.predict(df['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
